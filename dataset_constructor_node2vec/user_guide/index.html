
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>User Guide &#8212; sknet  documentation</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="API Reference" href="../api_reference/index.html" />
    <link rel="prev" title="Unsupervised Learning" href="../getting_started/unsupervised_learning.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    

<a class="navbar-brand" href="../index.html">
  <img src="../_static/logo.png" class="logo" alt="logo">
</a>


    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../getting_started/index.html">
  Getting Started
 </a>
</li>

<li class="toctree-l1 current active nav-item">
 <a class="current reference internal nav-link" href="#">
  User Guide
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../api_reference/index.html">
  API Reference
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../development/index.html">
  Development
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/TNanukem/scikit-net" rel="noopener" target="_blank" title="GitHub">
            <span><i class="fab fa-github-square"></i></span>
            <label class="sr-only">GitHub</label>
          </a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="https://twitter.com/TiagoJToledoJr" rel="noopener" target="_blank" title="Twitter">
            <span><i class="fab fa-twitter-square"></i></span>
            <label class="sr-only">Twitter</label>
          </a>
        </li>
      </ul>
      </div>
      
      <div class="navbar-end-item">
        <ul class="navbar-nav">
    <li class="mr-2 dropdown">
        <button type="button" class="btn btn-version btn-sm navbar-btn dropdown-toggle" id="dLabelMore" data-toggle="dropdown">
            v
            <span class="caret"></span>
        </button>
        <ul class="dropdown-menu" aria-labelledby="dLabelMore">
            <li><a href="https://tnanukem.github.io/scikit-net/develop/">develop (latest)</a></li>
            <li><a href="https://tnanukem.github.io/scikit-net/main/">main (stable)</a></li>
        </ul>
    </li>
</ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar"><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    
  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#transformation-methods">
   Transformation methods
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dataset-constructors">
     Dataset Constructors
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#knn-constructor">
       KNN Constructor
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#epsilon-radius-constructor">
       Epsilon-Radius Constructor
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#knn-epsilon-radius-constructor">
       KNN Epsilon-Radius Constructor
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#single-linkage-clustering-heuristics-constructor">
       Single Linkage Clustering Heuristics Constructor
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#time-series-constructors">
     Time Series Constructors
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#correlation-constructor">
       Correlation Constructor
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#recurrence-constructor">
       Recurrence Constructor
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#supervised-methods">
   Supervised Methods
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#heuristic-of-ease-of-access">
     Heuristic of Ease of Access
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#high-level-data-classification">
     High Level Data Classification
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#how-the-high-level-classification-is-done">
       How the high-level classification is done
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#unsupervised-methods">
   Unsupervised Methods
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#stochastic-particle-competition">
     Stochastic Particle Competition
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#semi-supervised-methods">
   Semi Supervised Methods
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#modularity-label-propagation">
     Modularity Label Propagation
    </a>
   </li>
  </ul>
 </li>
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                
              </div>
              
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <div class="section" id="user-guide">
<h1>User Guide<a class="headerlink" href="#user-guide" title="Permalink to this headline">Â¶</a></h1>
<p>This section will introduce the main modules of the sknet and show some examples as well as explaining the theory
behind the implemented algorithms.</p>
<p>The sknet main structure divide the classes into two main types: auxiliary methods such as utilities and transformations and
machine learning methods which are divided into supervised, unsupervised and semi supervised methods.</p>
<p>Most of the Machine Learning methods can work both with tabular data (in form of a Pandas Dataframe or a Numpy Array) and with graph data
(in form of a NetworkX complex network), exceptions will be explicit on the documentation.</p>
<div class="section" id="transformation-methods">
<h2>Transformation methods<a class="headerlink" href="#transformation-methods" title="Permalink to this headline">Â¶</a></h2>
<p>These are the backbones of the inner workings of the sknet. The transformation classes are responsible for transforming data from one
type to another. To this date, the following transformations are possible:</p>
<ul class="simple">
<li><p>Tabular Data -&gt; Complex Network</p></li>
<li><p>Time Series -&gt; Complex Network</p></li>
</ul>
<p>The Machine Learning classes are responsible for transforming data to the appropriate format for each one, however, one can always
insert the already transformed data into the class.</p>
<div class="section" id="dataset-constructors">
<h3>Dataset Constructors<a class="headerlink" href="#dataset-constructors" title="Permalink to this headline">Â¶</a></h3>
<p>Those are the methods responsible for transforming tabular data, from the Pandas DataFrame or the Numpy Array format into a
NetworkX complex network.</p>
<p>When dealing with Dataset Constructors, one may have the classes of the tabular data availabe (such as on a supervised method),
on that case, one may set the constructor so it will generate separated components for each class. Some Machine Learning models
will require this while others will require that no separated component is generated. Look up for the documentation of each method
to be aware of the requirements for each method.</p>
<div class="section" id="knn-constructor">
<h4>KNN Constructor<a class="headerlink" href="#knn-constructor" title="Permalink to this headline">Â¶</a></h4>
<p>The KNN Constructor uses a k-Nearest Neighbors algorithm to create edges between the instances (rows) of our tabular dataset. For that
the distance between each instance of the dataset is calculated using some distance metric, like the Euclidean Distance, and then, for each
instance, the k closest instances are selected and edges are created between them.</p>
<p>Notice that this methodology does not create a symmetric network since, given node <code class="docutils literal notranslate"><span class="pre">i</span></code>, node <code class="docutils literal notranslate"><span class="pre">j</span></code> could be one of the k closest points to it but
the contrary may not be true.</p>
<img alt="KNN Constructor" src="../_images/knn.png" />
<p>Also, this method does not allow for singletons to be created. If a node is too far away from the others on the generated space, it will
create at least k edges with k other nodes.</p>
<p>The main drawback of this methodology is that, for dense regions where there are too many nodes close to each other, the degree of each node
will be underestimated.</p>
</div>
<div class="section" id="epsilon-radius-constructor">
<h4>Epsilon-Radius Constructor<a class="headerlink" href="#epsilon-radius-constructor" title="Permalink to this headline">Â¶</a></h4>
<p>The Epsilon-Radius constructor, for each node (row) of the dataset, connects it to all nodes that are inside a circle of radius epsilon.  For that
the distance between each instance of the dataset is calculated using some distance metric, like the Euclidean Distance.</p>
<img alt="Epsilon Radius Constructor" src="../_images/epsilon.png" />
<p>This methodology will create a symmetric network since that, for a node to be inside the radius of another, the contrary must also be true at all times. However
this method allows singletons to be created since it may be that there are no nodes inside the radius, which is a big drawback of this method.</p>
</div>
<div class="section" id="knn-epsilon-radius-constructor">
<h4>KNN Epsilon-Radius Constructor<a class="headerlink" href="#knn-epsilon-radius-constructor" title="Permalink to this headline">Â¶</a></h4>
<p>To overcome the drawbacks of both, the KNN Constructor and the Epsilon-Radius constructor, the KNN Epsilon-Radius Constructor tries to sum-up the strenghts
of both methods. This constructor will use the Epsilon-Radius method for dense regions of the space and the K-NN method for sparse regions according to the
following equation:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\left\{\begin{matrix}
   \epsilon\text{-radius}(v_i), &amp; \text{if} |\epsilon\text{-radius}| &gt; k \\
   k\text{-NN}(v_i), &amp; \text{otherwise}
\end{matrix}\right.\end{split}\]</div>
<p>The idea behind this strategy is to add more edges on dense regions that should be more densely connected and to avoid singletons being created on sparse
regions. This way, the generated network will be connected and will have a variable degree level, better representing real world networks.</p>
<img alt="KNN Epsilon-Radius Constructor" src="../_images/k-eps.png" />
</div>
<div class="section" id="single-linkage-clustering-heuristics-constructor">
<h4>Single Linkage Clustering Heuristics Constructor<a class="headerlink" href="#single-linkage-clustering-heuristics-constructor" title="Permalink to this headline">Â¶</a></h4>
<p>This constructor uses the idea of the Single Linkage heuristic for clustering to generate a network that preserves the original clustering topology
of the dataset. This tries to avoid the over sparsity or over density of the generated networks from the previous constructors that are not able to
guarantee the maintainance of the cluster topology.</p>
<p>The first step is to calculate the distance between each instance of the dataset is calculated using some distance metric, like the Euclidean Distance.
With that in hands, each node is considered a cluster, then, the two closest clusters are found and the k nearest neighbors between them are connected
by edges if their distance is smaller than a threshold defined by the intra-cluster dissimilarity of each one.</p>
<p>This process merges the two clusters. Then, it repeats until we have only one cluster left, then the network is complete.</p>
<p>This method will keep the sparsity between clusters and the density inside a cluster, which, depending on the problem at hand, can be necessary
for the study of the data.</p>
<p>More information about this method can be found in the following paper:
Cupertino, T.H., Huertas, J., &amp; Zhao, L. (2013). Data clustering using controlled consensus in complex networks. Neurocomputing, 118, 132-140.</p>
</div>
</div>
<div class="section" id="time-series-constructors">
<h3>Time Series Constructors<a class="headerlink" href="#time-series-constructors" title="Permalink to this headline">Â¶</a></h3>
<p>Those are the methods responsible for transforming time series data, univariate or multivariate, into a complex network representation.</p>
<div class="section" id="correlation-constructor">
<h4>Correlation Constructor<a class="headerlink" href="#correlation-constructor" title="Permalink to this headline">Â¶</a></h4>
<p>The idea behind the Correlation Constructor is to split the time series into N segments of lenght L
each one which will be a node in our complex network. Then, having those segments, one can calculate the pearson correlation coefficient
between those segments, creating a correlation matrix C.</p>
<p>Then, an user-defined parameter <code class="docutils literal notranslate"><span class="pre">r</span></code> defines the correlation threshold for the creation of an edge between two nodes (segments) of the
network. If the correlation between them is greater than <code class="docutils literal notranslate"><span class="pre">r</span></code>, then an edge is created.</p>
<p>Notice that this generate an undirected graph since the correlation between two segments will always be symmetric. This module implements
two variations of this method: one for univariate time series and another for multivariate times series.</p>
<p>More information about those methods can be found on: Yang, Y., Yang, H.: Complex network-based time series analysis. Physica A 387, 1381â1386 (2008)</p>
</div>
<div class="section" id="recurrence-constructor">
<h4>Recurrence Constructor<a class="headerlink" href="#recurrence-constructor" title="Permalink to this headline">Â¶</a></h4>
<p>The recurrence constructor uses the concept of recurrence on the phase space of the time series. Given an embedding of the time series (such as the
Takens Embedding), it is said that two states are recurrent if they are similar enough. So, given two states in the phase space defined as:</p>
<div class="math notranslate nohighlight">
\[x_i = (x(t), x(t + \tau), \dots , x(t + (d - 1)\tau))\]</div>
<p>Two states are recurrent if:</p>
<div class="math notranslate nohighlight">
\[||x_i - x_j|| &lt; \epsilon\]</div>
<p>Then, after the embedding was made, one can easily calculate a distance matrix between each of the states. Then, the self-loops (diagonals) are
set to zero and every entry smaller than epsilon will generate an edge between the states of the series.</p>
<p>More information about this method can be found on: Donner, R.V., Zou, Y., Donges, J.F., Marwan, N., Kurths, J.: Recurrence
networks â a novel paradigm for nonlinear time series analysis. New J. Phys. 12, 033025 (2010)</p>
</div>
</div>
</div>
<div class="section" id="supervised-methods">
<h2>Supervised Methods<a class="headerlink" href="#supervised-methods" title="Permalink to this headline">Â¶</a></h2>
<p>Supervised methods have one objective: given a labeled dataset, learn the data patterns to the able to predict the label (continous or discrete)
of new, unseen, data samples.</p>
<div class="section" id="heuristic-of-ease-of-access">
<h3>Heuristic of Ease of Access<a class="headerlink" href="#heuristic-of-ease-of-access" title="Permalink to this headline">Â¶</a></h3>
<p>This algorithm can be used, both, as a classifier and as a regressor. Its main idea is to consider the network as a Markov Chain to, on the convergence
of the chain, identify which classes (or values) have a higher probability for a given unlabeled instance.</p>
<p>Given the network with labeled instances we have the weight matrix of the network, which can be considered as the adjacency matrix of a weighted network.</p>
<p>For each unlabeled instance we add it to the network and calculate the similarity (which in this case can be an Euclidean distance for example) of this
new node to every other node of the network which will be put into a vector <code class="docutils literal notranslate"><span class="pre">S</span></code>. Using those similarities, we will disturb the weights matrix, using
an parameter epsilon, according to the following formula:</p>
<div class="math notranslate nohighlight">
\[\hat{W} = W + \epsilon \hat{S}\]</div>
<p>where:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\hat{S} = \begin{bmatrix}
   s_1 &amp; \dots &amp; s_1 \\
   s_2 &amp; \dots &amp; s_2\\
   \vdots &amp; \vdots &amp; \vdots\\
   s_L &amp; \dots &amp; s_L
   \end{bmatrix}\end{split}\]</div>
<p>The image below shows the effect of adding this new node and removing if it right after. Notice that now self loops are created in the network since we
are summing up a value on every weight.</p>
<img alt="Ease of Access network change" src="../_images/ease_of_access.png" />
<p>Then, we use the weight matrix to calculate the transition probabilities and finally we compute the convergence of the Markov Chain to the limiting
probabilities. At this point, every limiting probability represents a state and can be interpreted as the probability of the unlabeled example
belonging to the class of that state.</p>
<p>We then select the <code class="docutils literal notranslate"><span class="pre">t</span></code> biggest probabilities to define the class of or unlabeled example. In case of a classification, the mode of the top
<code class="docutils literal notranslate"><span class="pre">t</span></code> states is considered. If we are dealing with a regression, then the average value of the <code class="docutils literal notranslate"><span class="pre">t</span></code> states is used.</p>
<p>More information about this method can be found on: Cupertino, T.H., Zhao, L., Carneiro, M.G.: Network-based supervised data
classification by using an heuristic of ease of access. Neurocomputing 149(Part A), 86â92 (2015)</p>
</div>
<div class="section" id="high-level-data-classification">
<h3>High Level Data Classification<a class="headerlink" href="#high-level-data-classification" title="Permalink to this headline">Â¶</a></h3>
<p>The High Level Data Classification algorithm tries to incorporate the findings from traditional Machine Learning algorithms, such as SVMs and
Random Forests, with the structural pattern recognition promoted by analyzing the metrics of a complex network. In order to do so, it receives
the tabular data in a regular Machine Learning fashion and fits a low-level (traditional ML) classifier on the data.</p>
<p>Then the dataset is transformed into a complex network with a separated component for each of its classes, using one of the available constructors.
This network is what we call the training network.</p>
<p>For each of the unlabeled examples we want to predict, two kind of predictions will be done:</p>
<blockquote>
<div><ul class="simple">
<li><p>A low-level prediction where the fitted low-level model will have its <code class="docutils literal notranslate"><span class="pre">predict</span></code> or <code class="docutils literal notranslate"><span class="pre">predict_proba</span></code> method called to classify the data.</p></li>
<li><p>A high-level prediction where we will use the complex network to calculate a probability of the instance belonging to any of the classes</p></li>
</ul>
</div></blockquote>
<p>Once this is done, the probability of allocation on each class is defined by the equation:</p>
<div class="math notranslate nohighlight">
\[F_i^{(y)} = (1 - \rho )L_i^{(y)} + \rho H_i^{(y)}\]</div>
<p>Where <span class="math notranslate nohighlight">\(\rho\)</span> is a user-defined parameter, <span class="math notranslate nohighlight">\(F_i^{(y)}\)</span> is the probability of <span class="math notranslate nohighlight">\(i\)</span> belonging to class y, <span class="math notranslate nohighlight">\(L_i^{(y)}\)</span> is the probabilities
associated with the low-level classifier and <span class="math notranslate nohighlight">\(H_i^{(y)\)</span> are the probabilities associated with the high-level classifier.</p>
<div class="section" id="how-the-high-level-classification-is-done">
<h4>How the high-level classification is done<a class="headerlink" href="#how-the-high-level-classification-is-done" title="Permalink to this headline">Â¶</a></h4>
<p>In order to generate the probabilities from <span class="math notranslate nohighlight">\(H_i^{(y)}\)</span>, each unlabeled example is inserted into each of the components of the network, in
which case we are basically testing it on every class of our data. Then, several metrics are calculated on the network, before and after the
insertion of this new data point.</p>
<p>If this insertion changes those metrics too much, this is an evidence that maybe it does not belong in that class. On the other hand, if the
metrics remain almost constant, it means that this new example does not change the structure of the network and thus may be part of that class.</p>
<p>The user can define which metrics will be computed and what is the weight to be used on each metric by means of the list of parameters <span class="math notranslate nohighlight">\(\alpha\)</span>).
Notice that <span class="math notranslate nohighlight">\(\alpha\)</span> must sum-up to 1.</p>
<p>The list of available functions can be seem on the documentation of the NetworkMetrics (colocar link aqui). More information about this method can be
found on: Silva, T.C., Zhao, L.: Network-based high level data classification. IEEE Trans. Neural Netw. Learn. Syst. 23(6), 954â970 (2012)</p>
</div>
</div>
</div>
<div class="section" id="unsupervised-methods">
<h2>Unsupervised Methods<a class="headerlink" href="#unsupervised-methods" title="Permalink to this headline">Â¶</a></h2>
<p>Unsupervised methods, usually called community detection methods on the Complex Network area, are algorithms that try to find patterns on
the data so to group up data samples.</p>
<div class="section" id="stochastic-particle-competition">
<h3>Stochastic Particle Competition<a class="headerlink" href="#stochastic-particle-competition" title="Permalink to this headline">Â¶</a></h3>
<p>The Stochastic Particle Competition algorithm lends some of the concepts of the genetic algorithms optimization to find community structure
on complex networks. Given a set of <code class="docutils literal notranslate"><span class="pre">K</span></code> initial particles, put at random on the nodes of the network, they will compete against each other
for the dominance of the network nodes. It is expected that after some time this algorithm will converge to a state where each community is
dominated by one of the initial <code class="docutils literal notranslate"><span class="pre">K</span></code> particles.</p>
<p>At each timestep, each particle chooses the the next node to visit by combinating a preferential movement matrix, where it has a greater
probability of visiting previously visited nodes, and a exploration matrix, which will send this particle over to new areas in order to try
to dominate them.</p>
<p>The <span class="math notranslate nohighlight">\(\lambda\)</span> parameter is responsible to define how much exploration versus exploitation each of the particles will do during the fitting process.</p>
<p>Each time one node is visited by a particle, its dominance on the node increases. The same way, if a rival particle visits the same node, then
the dominance level will be reduced. On the same way, every time a particle visits a dominated node, it regains energy, while if it visits a
node dominated by other particle, it loses energy. If a particle runs out of energy, then it is transported back to its dominance region.</p>
<p>The minimal and maximal energy of each particle is defined by the <span class="math notranslate nohighlight">\(\omega_{min}\)</span> and <span class="math notranslate nohighlight">\(\omega_{max}\)</span> parameters respectively.</p>
<p>The convergence of the system happens when the difference between the dominance levels on two sequential steps is smaller than a user-defined
parameter <span class="math notranslate nohighlight">\(\epsilon\)</span>.</p>
<p>More information about this method can be found on: T. C. Silva and L. Zhao, âStochastic Competitive Learning in Complex
Networks,â in IEEE Transactions on Neural Networks and Learning Systems, vol. 23, no. 3, pp. 385-398, March 2012, doi: 10.1109/TNNLS.2011.2181866.</p>
</div>
</div>
<div class="section" id="semi-supervised-methods">
<h2>Semi Supervised Methods<a class="headerlink" href="#semi-supervised-methods" title="Permalink to this headline">Â¶</a></h2>
<p>These are methods designed to work with large amounts of unlabeled data given a small amount of labeled data. Usually this kind of method
works towards spreading labels from labeled examples to unlabeled examples.</p>
<div class="section" id="modularity-label-propagation">
<h3>Modularity Label Propagation<a class="headerlink" href="#modularity-label-propagation" title="Permalink to this headline">Â¶</a></h3>
<p>This algorithm is based on the greedy modularity maximization community detection algorithm. In order to use it, with need a dataset with <code class="docutils literal notranslate"><span class="pre">L</span></code>
labeled nodes and several unlabeled nodes. At each step of this algorithm, two communities (nodes) are merged to the same class following some
restrictions, trying to keep the modularity increment the maximum as possible.</p>
<p>The criteria for the merge at each step is as follows:</p>
<ul class="simple">
<li><p>If both nodes already have a class and are from different classes, the merge does not occour</p></li>
<li><p>If none of the nodes have a class, the merge does not occour</p></li>
<li><p>If the nodes have the same class, the merge occours</p></li>
<li><p>If one of the nodes have a class and the other doesnât, the merge occours</p></li>
</ul>
<p>If we werenât able to merge the pair of nodes with greatest value on the modularity increment matrix <span class="math notranslate nohighlight">\(\Delta Q\)</span>, we select the next
greatest value and so on until a valid merge takes place.</p>
<p>The algorithm runs until there is no node without a class remaining. The original paper of this algorithm states a network reduction technique to
improve the algorithms performance. In order to use it, the reduction_factor list parameter should be set during
the class instantiation.</p>
<p>This parameter will define, for each class, the percentage of the network reduction. The basic working of the method is:</p>
<ul class="simple">
<li><p>Select two nodes from the same class at random</p></li>
<li><p>Remove the first one</p></li>
<li><p>Redirects the edges from the first node to the second</p></li>
<li><p>Repeat until the desired percentage of the nodes are removed</p></li>
</ul>
<p>More information about this method can be found on: Silva, Thiago &amp; Zhao, Liang. (2012). Semi-Supervised Learning Guided
by the Modularity Measure in Complex Networks. Neurocomputing. 78. 30-37. 10.1016/j.neucom.2011.04.042.</p>
</div>
</div>
</div>


              </div>
              
              
              <div class='prev-next-bottom'>
                
    <a class='left-prev' id="prev-link" href="../getting_started/unsupervised_learning.html" title="previous page">Unsupervised Learning</a>
    <a class='right-next' id="next-link" href="../api_reference/index.html" title="next page">API Reference</a>

              </div>
              
          </main>
          

      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  <footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2022, Tiago Toledo.<br/>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.1.2.<br/>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>